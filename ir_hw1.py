# -*- coding: utf-8 -*-
"""IR_HW1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1eJrS0XcQOIAbsIolL1CVe2lNnVUuZtB0
"""

import numpy as np
import nltk
import pandas as pd
import re

!pip install hazm --quiet

import hazm

"""#Persian Dataset"""

!wget https://github.com/mohamad-dehghani/persian-pdf-books-dataset/raw/master/final_books.xlsx

df = pd.read_excel('/content/final_books.xlsx')
df.info()

df.head()

df.iloc[440:480]

df['content'].fillna(df['title'], inplace=True)

null_counts = df.isnull().sum()
null_counts

df['concat'] = df['title'] + ' ' + df['date'] + ' ' + df['content'] + ' ' + df['category'] + ' ' + df['author'] + ' ' + df['comments']
df

from hazm import Normalizer, Stemmer, stopwords_list, WordTokenizer

normalizer = Normalizer()
stemmer = Stemmer()
persian_stopwords = set(stopwords_list())

def tokenize_persian(text):
    text = remove_punctuation_persian(text)
    tokenizer = hazm.WordTokenizer()
    tokens = tokenizer.tokenize(text)
    return tokens

def remove_punctuation_persian(text):
    text = re.sub(r'_', '\u200c', text)
    punctuation_pattern = r'[^\w\s]+'
    text = re.sub(punctuation_pattern, ' ', text)
    text = text.strip()

    return text

def normalize_persian(tokens):
    tokens = [normalizer.normalize(token) for token in tokens]
    return tokens

def stemming(tokens):
    tokens = [stemmer.stem(token) for token in tokens]
    return tokens

def remove_stopwords(tokens):
    tokens = [token for token in tokens if token not in persian_stopwords]
    return tokens

df['tokenized_content'] = df['concat'].apply(tokenize_persian)
df['tokenized_content'] = df['tokenized_content'].apply(remove_stopwords)
df['normalized_content'] = df['tokenized_content'].apply(normalize_persian)
df['stemming_content'] = df['normalized_content'].apply(stemming)

print(df.loc[0,'tokenized_content'])

print(df.loc[0,'normalized_content'])

print(df.loc[0,'stemming_content'])

df.iloc[0]

from collections import defaultdict

def create_persian_inverted_index(df):
  subset_dataset = df.head(1000)

  inverted_index = defaultdict(list)
  term_dictionary = defaultdict(int)

  for index, row in subset_dataset.iterrows():
      document_id = index + 1
      normalized_text = row['stemming_content']

      term_count = defaultdict(int)
      for token in normalized_text:
          term_count[token] += 1

      for term, count in term_count.items():
          if term not in term_dictionary:
              term_dictionary[term] = 0
          term_dictionary[term] += count

          inverted_index[term].append((document_id, count))

  for term, postings in inverted_index.items():
      inverted_index[term] = sorted(postings, key=lambda x: x[1], reverse=True)


  with open('inverted_index_persian.txt', 'w') as file:
    for term, postings in inverted_index.items():
        term_freq = term_dictionary[term]
        posting_list = [(doc_id, count) for doc_id, count in postings]
        output_line = f"({term}) : frequency = {term_freq}, posting list: {posting_list}\n"
        print(f"({term}) : frequency = {term_freq}, posting list: {posting_list}")
        file.write(output_line)

  return inverted_index

def create_terms_binary(inverted_index):
  terms_binary = {}

  unique_docs = set(doc_id for postings in inverted_index.values() for _, doc_id in postings)

  for term, postings in inverted_index.items():
      term_occurrence_list = [1 if doc_id in {doc_id for _, doc_id in postings} else 0 for doc_id in unique_docs]
      terms_binary[term] = term_occurrence_list

  return terms_binary

import re

def persian_query_processing(query):
    pattern = r'[^\.\+]+|[\.\+]'
    matches = re.findall(pattern, query)

    terms = []
    operators = []

    for match in matches:
        if match in ('.', '+'):
            operators.append(match)
        else:
            terms.append(match)

    for i in terms:
      print(i)

    print("List of terms:", terms)
    print("List of operators:", operators)

    return terms, operators

def process_term_in_query_persian(term):
  tokenized = tokenize_persian(term)
  normalized = normalize_persian(tokenized)
  stem = stemming(normalized)

  return stem

def boolean_retrieval(terms_binary, terms, operators):

  length_of_values = max(len(value) for value in terms_binary.values())

  cur_term = terms[0]
  is_not = 0

  print('term: ', cur_term)

  if cur_term.startswith('!'):
    cur_term = cur_term[1:]
    is_not = 1

  processed_term = process_term_in_query_persian(cur_term)[0]
  print('processed term : ', processed_term)

  if processed_term in terms_binary:
    term_binary = terms_binary.get(processed_term, '')
    term_binary = ''.join(map(str, term_binary))
  else:
    term_binary = '0'.zfill(length_of_values)
    print('(not found)')

  if is_not == 1:
    term_binary = ''.join(['1' if bit == '0' else '0' for bit in term_binary])
    print('(NOT)')

  print('term binary : ', term_binary)

  result = term_binary


  for i in range(1, len(terms)):
      is_not = 0
      term = terms[i]
      operator = operators[i - 1]
      print('term : ', term)

      if term.startswith('!'):
        term = term[1:]
        is_not = 1

      processed_term = process_term_in_query_persian(term)[0]

      print('processed term : ', processed_term)


      if processed_term in terms_binary:
        term_binary = terms_binary.get(processed_term, '')
        term_binary = ''.join(map(str, term_binary))
      else:
        term_binary = '0'.zfill(length_of_values)
        print('(not found)')

      if is_not == 1:
        term_binary = ''.join(['1' if bit == '0' else '0' for bit in term_binary])
        print('(NOT)')

      print('term binary : ', term_binary)

      if operator == '+':
          result = "".join(['1' if a == '1' or b == '1' else '0' for a, b in zip(result, term_binary)])
      elif operator == '.':
          result = "".join(['1' if a == '1' and b == '1' else '0' for a, b in zip(result, term_binary)])

  document_numbers = [i + 1 for i, bit in enumerate(result) if bit == '1']

  print("Result:", result)
  print("Document Numbers:", document_numbers)

  return result, document_numbers

import time
start_time = time.time()
persian_inverted_index = create_persian_inverted_index(df)
end_time = time.time()
execution_time = end_time - start_time
print(f"زمان اجرا: {execution_time} ثانیه")

def calculate_statistics_Persian(df):
    total_tokens = 0
    for index, row in df.iterrows():
        normalized_text = row['stemming_content']
        total_tokens += len(normalized_text)

    total_documents = len(df)
    total_file_size = df['stemming_content'].apply(lambda x: len(x)).sum()

    file_path = "/content/inverted_index_persian.txt"
    if os.path.exists(file_path):
      file_size = os.path.getsize(file_path)
    else:
      print("file not found")

    return total_tokens, total_documents, total_file_size,file_size

total_tokens, total_documents, total_file_size, file_size = calculate_statistics_Persian(df)
print(f"Total Tokens: {total_tokens}")
print(f"Total Documents: {total_documents}")
print(f"Total File Size: {total_file_size} bytes")
print(f"The size of Persian inverted index file: {file_size} bytes")

persian_terms_binary = create_terms_binary(persian_inverted_index)

query = input("Enter query ... : ")
print(query)

terms, operators = persian_query_processing(query)
result, document_numbers = boolean_retrieval(persian_terms_binary, terms, operators)

file_path = "persian_queries.txt"

query_number = "1"

result_list = [str(item) for item in document_numbers]

result_list_str = ','.join(result_list)


with open(file_path, 'w') as file:
    file.write(query_number + ',')
    file.write(result_list_str)

query_number = "12"

result_list = [str(item) for item in document_numbers]

result_list_str = ','.join(result_list)

with open(file_path, 'a') as file:
    file.write('\n')
    file.write(query_number + ',')
    file.write(result_list_str)

"""#English Dataset"""

!wget http://www.cs.cmu.edu/~ark/personas/data/MovieSummaries.tar.gz
!tar -xf MovieSummaries.tar.gz

df2 = pd.read_csv("/content/MovieSummaries/plot_summaries.txt", delimiter = "\t",names=["id","text"])
df2.info()

df2.head()

df2.iloc[333:343]

nltk.download('punkt')

null_counts = df2.isnull().sum()
null_counts

from nltk.stem import PorterStemmer
from nltk.corpus import stopwords

nltk.download('stopwords')
stemmer = PorterStemmer()
english_stopwords = set(nltk.corpus.stopwords.words('english'))

def tokenize_english(text):
    tokens = nltk.word_tokenize(text)
    return tokens

def normalize_english_text(tokenized_text):
    tokenized_text = [token.lower() for token in tokenized_text]
    tokenized_text = [remove_punctuation(token) for token in tokenized_text]
    return tokenized_text

def removeStopwords_English(tokens):
    tokens = [token for token in tokens if token.isalpha() and token not in english_stopwords]
    return tokens

def stem_english(tokens):
    tokens = [stemmer.stem(token) for token in tokens]
    return tokens

def remove_punctuation(tokenized_text):
    tokens = nltk.word_tokenize(tokenized_text)
    tokens = [token for token in tokens if token.isalpha()]
    return ' '.join(tokens)

df2['tokenized_text'] = df2['text'].apply(tokenize_english)
df2['tokenized_text'] = df2['tokenized_text'].apply(removeStopwords_English)
df2['normalized_text'] = df2['tokenized_text'].apply(normalize_english_text)
df2['stemming_text'] = df2['normalized_text'].apply(stem_english)

df2.loc[0,'text']

df2.loc[0,'tokenized_text']

df2.loc[0,'normalized_text']

df2.loc[0,'stemming_text']

df2.iloc[0]

def create_english_inverted_index(df):
  subset_dataset_english = df.head(1000)

  inverted_index_english = defaultdict(list)
  term_dictionary_english = defaultdict(int)

  for index, row in subset_dataset_english.iterrows():
      document_id = index + 1
      normalized_text = row['stemming_text']

      term_count = defaultdict(int)
      for token in normalized_text:
          term_count[token] += 1

      for term, count in term_count.items():
          if term not in term_dictionary_english:
              term_dictionary_english[term] = 0
          term_dictionary_english[term] += count

          inverted_index_english[term].append((document_id, count))

  for term, postings in inverted_index_english.items():
      inverted_index_english[term] = sorted(postings, key=lambda x: x[1], reverse=True)

  with open('inverted_index_english.txt', 'w') as file:
    for term, postings in inverted_index_english.items():
        term_freq = term_dictionary_english[term]
        posting_list = [(doc_id, count) for doc_id, count in postings]
        output_line = f"({term}) : frequency = {term_freq}, posting list: {posting_list}\n"
        print(f"({term}) : frequency = {term_freq}, posting list: {posting_list}")
        file.write(output_line)
  return inverted_index_english

start_time = time.time()
english_inverted_index = create_english_inverted_index(df2)
end_time = time.time()
execution_time = end_time - start_time
print(f"زمان اجرا: {execution_time} ثانیه")

import os
def calculate_statistics_english(df):
    total_tokens = 0
    for index, row in df.iterrows():
        normalized_text = row['stemming_text']
        total_tokens += len(normalized_text)

    total_documents = len(df)
    total_file_size = df['stemming_text'].apply(lambda x: len(x)).sum()
    file_path = "/content/inverted_index_english.txt"
    if os.path.exists(file_path):
      file_size = os.path.getsize(file_path)
    else:
      print("file not found")

    return total_tokens, total_documents, total_file_size,file_size

total_tokens, total_documents, total_file_size, file_size = calculate_statistics_english(df2)
print(f"Total Tokens: {total_tokens}")
print(f"Total Documents: {total_documents}")
print(f"Total File Size: {total_file_size} bytes")
print(f"The size of English inverted index file: {file_size} bytes")

english_terms_binary = create_terms_binary(english_inverted_index)

def process_term_in_query_english(term):
  tokenized = tokenize_english(term)
  normalized = normalize_english_text(tokenized)
  stem = stem_english(normalized)

  return stem

def boolean_retrieval(terms_binary, terms, operators):


  length_of_values = max(len(value) for value in terms_binary.values())


  cur_term = terms[0]
  is_not = 0

  print('term: ', cur_term)

  if cur_term.startswith('!'):
    cur_term = cur_term[1:]
    is_not = 1

  processed_term = process_term_in_query_english(cur_term)[0]
  print('processed term : ', processed_term)

  if processed_term in terms_binary:
    term_binary = terms_binary.get(processed_term, '')
    term_binary = ''.join(map(str, term_binary))
  else:
    term_binary = '0'.zfill(length_of_values)
    print('(not found)')

  if is_not == 1:
    term_binary = ''.join(['1' if bit == '0' else '0' for bit in term_binary])
    print('(NOT)')

  print('term binary : ', term_binary)

  result = term_binary


  for i in range(1, len(terms)):
      is_not = 0
      term = terms[i]
      operator = operators[i - 1]
      print('term : ', term)

      if term.startswith('!'):
        term = term[1:]
        is_not = 1

      processed_term = process_term_in_query_persian(term)[0]

      print('processed term : ', processed_term)


      if processed_term in terms_binary:
        term_binary = terms_binary.get(processed_term, '')
        term_binary = ''.join(map(str, term_binary))
      else:
        term_binary = '0'.zfill(length_of_values)
        print('(not found)')

      if is_not == 1:
        term_binary = ''.join(['1' if bit == '0' else '0' for bit in term_binary])
        print('(NOT)')

      print('term binary : ', term_binary)

      if operator == '+':
          result = "".join(['1' if a == '1' or b == '1' else '0' for a, b in zip(result, term_binary)])
      elif operator == '.':
          result = "".join(['1' if a == '1' and b == '1' else '0' for a, b in zip(result, term_binary)])

  document_numbers = [i + 1 for i, bit in enumerate(result) if bit == '1']

  print("Result:", result)
  print("Document Numbers:", document_numbers)

  return result, document_numbers

def english_query_processing(query):

  pattern = r'[\w!]+|[\+\.\*]'
  matches = re.findall(pattern, query)

  terms = []
  operators = []

  for match in matches:
      if match in ('+', '.'):
          operators.append(match)
      else:
          terms.append(match)

  print("List of terms:", terms)
  print("List of operators:", operators)

  return terms, operators

query = input("Enter query ... : ")
print(query)

terms, operators = english_query_processing(query)
result, document_numbers = boolean_retrieval(english_terms_binary, terms, operators)

file_path = "english_queries.txt"

query_number = "1"

result_list = [str(item) for item in document_numbers]

result_list_str = ','.join(result_list)


with open(file_path, 'w') as file:
    file.write(query_number + ',')
    file.write(result_list_str)

query_number = "13"

result_list = [str(item) for item in document_numbers]

result_list_str = ','.join(result_list)

with open(file_path, 'a') as file:
    file.write('\n')
    file.write(query_number + ',')
    file.write(result_list_str)